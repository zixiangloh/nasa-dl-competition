{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf378af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: https://www.drivendata.co/blog/predict-pm25-benchmark/\n",
    "\n",
    "pyhdf appears to be more powerful than gdal, so it may be worth adopting some of the \n",
    "methods used here for working with hdf files.\n",
    "\n",
    "Additionally, the tutorial shows how to make a masked numpy array, which allows us to work\n",
    "with sparse arrays? (I'm not sure how this works yet.)\n",
    "\n",
    "Finally, the tutorial explains how to align AOD data with coordinates. This could let us\n",
    "make some useful model features, like local weather conditions, etc.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "import keras.backend as backend\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from dateutil import parser\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhdf.SD import SD, SDC, SDS\n",
    "import pyproj\n",
    "from pyproj import CRS, Proj\n",
    "from typing import Union\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "# from pathlib import Path\n",
    "# import random\n",
    "# from typing import Dict, List, Union\n",
    "\n",
    "# from cloudpathlib import S3Path\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# DATA_PATH = Path.cwd().parent / \"data\"\n",
    "# RAW = DATA_PATH / \"raw\"\n",
    "# INTERIM = DATA_PATH / \"interim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74f276b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDATA PROCESSING\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATA PROCESSING\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc06cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over orbits to apply the attributes\n",
    "def calibrate_data(dataset: SDS, shape: list[int], calibration_dict: dict):\n",
    "    \"\"\"Given a MAIAC dataset and calibration parameters, return a masked\n",
    "    array of calibrated data.\n",
    "    \n",
    "    Args:\n",
    "        dataset (SDS): dataset in SDS format (e.g. blue band AOD).\n",
    "        shape (List[int]): dataset shape as a list of [orbits, height, width].\n",
    "        calibration_dict (Dict): dictionary containing, at a minimum,\n",
    "            `valid_range` (list or tuple), `_FillValue` (int or float),\n",
    "            `add_offset` (float), and `scale_factor` (float).\n",
    "    \n",
    "    Returns:\n",
    "        corrected_AOD (np.ma.MaskedArray): masked array of calibrated data\n",
    "            with a fill value of nan.\n",
    "    \"\"\"\n",
    "    corrected_AOD = np.ma.empty(shape, dtype=np.double)\n",
    "    for orbit in range(shape[0]):\n",
    "        data = dataset[orbit, :, :].astype(np.double)\n",
    "        invalid_condition = (\n",
    "            (data < calibration_dict[\"valid_range\"][0]) |\n",
    "            (data > calibration_dict[\"valid_range\"][1]) |\n",
    "            (data == calibration_dict[\"_FillValue\"])\n",
    "        )\n",
    "        data[invalid_condition] = np.nan\n",
    "        data = (\n",
    "            (data - calibration_dict[\"add_offset\"]) *\n",
    "            calibration_dict[\"scale_factor\"]\n",
    "        )\n",
    "        data = np.ma.masked_array(data, np.isnan(data))\n",
    "        corrected_AOD[orbit, : :] = data\n",
    "    corrected_AOD.fill_value = np.nan\n",
    "    return corrected_AOD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4bdf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Aligning AOD data with real world coordinates\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_meshgrid(alignment_dict: dict, shape: list[int]):\n",
    "    \"\"\"Given an image shape, create a meshgrid of points\n",
    "    between bounding coordinates.\n",
    "    \n",
    "    Args:\n",
    "        alignment_dict (Dict): dictionary containing, at a minimum,\n",
    "            `upper_left` (tuple), `lower_right` (tuple), `crs` (str),\n",
    "            and `crs_params` (tuple).\n",
    "        shape (List[int]): dataset shape as a list of\n",
    "            [orbits, height, width].\n",
    "    \n",
    "    Returns:\n",
    "        xv (np.array): x (longitude) coordinates.\n",
    "        yv (np.array): y (latitude) coordinates.\n",
    "    \"\"\"\n",
    "    # Determine grid bounds using two coordinates\n",
    "    x0, y0 = alignment_dict[\"upper_left\"]\n",
    "    x1, y1 = alignment_dict[\"lower_right\"]\n",
    "    \n",
    "    # Interpolate points between corners, inclusive of bounds\n",
    "    x = np.linspace(x0, x1, shape[2], endpoint=True)\n",
    "    y = np.linspace(y0, y1, shape[1], endpoint=True)\n",
    "    \n",
    "    # Return two 2D arrays representing X & Y coordinates of all points\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    return xv, yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffe6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Source: https://spatialreference.org/ref/sr-org/modis-sinusoidal/proj4js/\n",
    "\n",
    "def transform_arrays(\n",
    "    xv: Union[np.array, float],\n",
    "    yv: Union[np.array, float],\n",
    "    crs_from: CRS,\n",
    "    crs_to: CRS\n",
    "):\n",
    "    \"\"\"Transform points or arrays from one CRS to another CRS.\n",
    "    \n",
    "    Args:\n",
    "        xv (np.array or float): x (longitude) coordinates or value.\n",
    "        yv (np.array or float): y (latitude) coordinates or value.\n",
    "        crs_from (CRS): source coordinate reference system.\n",
    "        crs_to (CRS): destination coordinate reference system.\n",
    "    \n",
    "    Returns:\n",
    "        lon, lat (tuple): x coordinate(s), y coordinate(s)\n",
    "    \"\"\"\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        crs_from,\n",
    "        crs_to,\n",
    "        always_xy=True,\n",
    "    )\n",
    "    \n",
    "    lon, lat = transformer.transform(xv, yv)\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "\n",
    "# Project sinu grid onto wgs84 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22605e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b97d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently not used\n",
    "def convert_array_to_df(\n",
    "    corrected_arr: np.ma.MaskedArray,\n",
    "    lat:np.ndarray,\n",
    "    lon: np.ndarray,\n",
    "    granule_id: str,\n",
    "    crs: CRS,\n",
    "    total_bounds: np.ndarray = None\n",
    "):\n",
    "    \"\"\"Align data values with latitude and longitude coordinates\n",
    "    and return a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        corrected_arr (np.ma.MaskedArray): data values for each pixel.\n",
    "        lat (np.ndarray): latitude for each pixel.\n",
    "        lon (np.ndarray): longitude for each pixel.\n",
    "        granule_id (str): granule name.\n",
    "        crs (CRS): coordinate reference system\n",
    "        total_bounds (np.ndarray, optional): If provided,\n",
    "            will filter out points that fall outside of these bounds.\n",
    "            Composed of xmin, ymin, xmax, ymax.\n",
    "    \"\"\"\n",
    "    lats = lat.ravel()\n",
    "    lons = lon.ravel()\n",
    "    n_orbits = len(corrected_arr)\n",
    "    size = lats.size\n",
    "    values = {\n",
    "        \"value\": np.concatenate([d.data.ravel() for d in corrected_arr]),\n",
    "        \"lat\": np.tile(lats, n_orbits),\n",
    "        \"lon\": np.tile(lons, n_orbits),\n",
    "        \"orbit\": np.arange(n_orbits).repeat(size),\n",
    "        \"granule_id\": [granule_id] * size * n_orbits\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(values).dropna()\n",
    "    if total_bounds is not None:\n",
    "        x_min, y_min, x_max, y_max = total_bounds\n",
    "        df = df[df.lon.between(x_min, x_max) & df.lat.between(y_min, y_max)]\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    gdf[\"geometry\"] = gpd.points_from_xy(gdf.lon, gdf.lat)\n",
    "    gdf.crs = crs\n",
    "    return gdf[[\"granule_id\", \"orbit\", \"geometry\", \"value\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d36508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Some more helpful functions from the tutorial\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_calibration_dict(data: SDS):\n",
    "    \"\"\"Define calibration dictionary given a SDS dataset,\n",
    "    which contains:\n",
    "        - name\n",
    "        - scale factor\n",
    "        - offset\n",
    "        - unit\n",
    "        - fill value\n",
    "        - valid range\n",
    "    \n",
    "    Args:\n",
    "        data (SDS): dataset in the SDS format.\n",
    "    \n",
    "    Returns:\n",
    "        calibration_dict (Dict): dict of calibration parameters.\n",
    "    \"\"\"\n",
    "    return data.attributes()\n",
    "\n",
    "\n",
    "def create_alignment_dict(hdf: SD):\n",
    "    \"\"\"Define alignment dictionary given a SD data file, \n",
    "    which contains:\n",
    "        - upper left coordinates\n",
    "        - lower right coordinates\n",
    "        - coordinate reference system (CRS)\n",
    "        - CRS parameters\n",
    "    \n",
    "    Args:\n",
    "        hdf (SD): hdf data object\n",
    "    \n",
    "    Returns:\n",
    "        alignment_dict (Dict): dict of alignment parameters.\n",
    "    \"\"\"\n",
    "    group_1 = hdf.attributes()[\"StructMetadata.0\"].split(\"END_GROUP=GRID_1\")[0]\n",
    "    hdf_metadata = dict([x.split(\"=\") for x in group_1.split() if \"=\" in x])\n",
    "    alignment_dict = {\n",
    "        \"upper_left\": eval(hdf_metadata[\"UpperLeftPointMtrs\"]),\n",
    "        \"lower_right\": eval(hdf_metadata[\"LowerRightMtrs\"]),\n",
    "        \"crs\": hdf_metadata[\"Projection\"],\n",
    "        \"crs_params\": eval(hdf_metadata[\"ProjParams\"])\n",
    "    }\n",
    "    \n",
    "    return alignment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833abcbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Everything here is original code that uses the functions from the tutorial.\n",
    "\n",
    "within(): taken from https://automating-gis-processes.github.io/2017/lessons/L3/point-in-polygon.html\n",
    "\n",
    "Make_Submatrix(): A function which takes raw AOD matrix and a Grid ID of interest as input and outputs a submatrix \n",
    "of AOD values which are inside this grid point (5km by 5km).\n",
    "\n",
    "Currently, Make_Submatrix() only returns the number of pixels in the AOD matrix are within the location determined\n",
    "by Grid ID.\n",
    "\n",
    "The rest of the code in this cell runs extremely slowly, but this is because we are running it for all possible \n",
    "combinations of HDF file and Grid ID. When we actually use these functions to run a model on a given Grid ID and \n",
    "datetime, we will first filter the set of HDF files such that we only search through HDF files with matching city \n",
    "and matching datetime.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Helper function\n",
    "def Make_Poly(polyString):\n",
    "    poly_coords = []\n",
    "    for string in polyString.split(','):\n",
    "        split_string = string.split(' ')\n",
    "        if split_string[0] == 'POLYGON':\n",
    "            split_string = split_string[1:]\n",
    "            split_string[0] = str(split_string[0])[2:]\n",
    "    #         print(tuple(float(x) for x in split_string))\n",
    "        elif split_string[0] == '':\n",
    "            split_string = split_string[1:]\n",
    "        if split_string[1][-2] == ')':\n",
    "            split_string[1] = split_string[1][0:-2]\n",
    "        poly_coords.append(tuple(float(x) for x in split_string))\n",
    "\n",
    "    return Polygon(poly_coords)\n",
    "\n",
    "\n",
    "\n",
    "#Main function\n",
    "def Make_Submatrix(corrected_AOD, lon, lat, alignment_dict, grid_md, gridID):\n",
    "    \n",
    "    poly = Make_Poly(grid_md['wkt'][gridID])\n",
    "    return_list = []\n",
    "    for band in range(len(corrected_AOD)):\n",
    "    \n",
    "        \n",
    "        counter = 0\n",
    "        triples_array = []\n",
    "        for i in range(len(corrected_AOD[0])):\n",
    "            if lat[i,0] > poly.bounds[3]:\n",
    "                continue\n",
    "                \n",
    "            if lat[i,0] < poly.bounds[1]:\n",
    "                continue\n",
    "    \n",
    "            for j in range(len(corrected_AOD[0][0])):\n",
    "                if lon[i,j] > poly.bounds[2]:\n",
    "                    continue\n",
    "                p1 = Point(lon[i,j], lat[i,j]) \n",
    "                if(p1.within(poly)):\n",
    "                    triples_array.append((i, j, corrected_AOD[band,i,j]))\n",
    "                    counter+=1\n",
    "                \n",
    "                \n",
    "        if len(triples_array) == 0:\n",
    "            continue\n",
    "\n",
    "        temp_array = np.zeros((10,10))\n",
    "        temp_array = np.ma.masked_array(temp_array, mask=np.ones((10,10)))\n",
    "\n",
    "        i_array = [x[0] for x in triples_array]\n",
    "        j_array = [x[1] for x in triples_array]\n",
    "        min_i = min(i_array)\n",
    "        min_j = min(j_array)\n",
    "\n",
    "        \n",
    "\n",
    "        for triple in triples_array:\n",
    "            if triple[2] is np.ma.masked:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                temp_array[triple[0]-min_i, triple[1]-min_j] = triple[2]\n",
    "        return_list.append(temp_array)\n",
    "        \n",
    "    return return_list \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38afb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that each AOD_array in array_of_AOD_arrays should be a precalculated subarray corresponding to grid id\n",
    "def collect_features(array_of_AOD_arrays, area_per_subarray):\n",
    "    \n",
    "    total_values = len(array_of_AOD_arrays)*area_per_subarray\n",
    "    \n",
    "    all_values = np.zeros((total_values))\n",
    "    all_values = np.ma.masked_array(all_values, mask=np.ones((total_values)))\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for AOD_array in array_of_AOD_arrays:\n",
    "        for row in AOD_array:\n",
    "            for value in row:\n",
    "                if not (value is np.ma.masked):\n",
    "                    all_values[counter] = value\n",
    "                    counter+=1\n",
    "\n",
    "    \n",
    "    mean = np.ma.mean(all_values)\n",
    "    minimum = np.ma.min(all_values)\n",
    "    maximum = np.ma.max(all_values)\n",
    "    std = np.ma.std(all_values)\n",
    "    summ = np.ma.sum(all_values)\n",
    "    \n",
    "    print('done')\n",
    "    return(mean, minimum, maximum, std, summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d8b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(train_labels, satellite_metadata, grid_id_list, training):\n",
    "    features = []\n",
    "    for i in range(len(train_labels)):\n",
    "        \n",
    "        print(i)\n",
    "#         print(features)\n",
    "\n",
    "        satellite_metadata_cut = satellite_metadata\n",
    "\n",
    "        grid_id = train_labels['grid_id'][i]\n",
    "        j = grid_id_list.index(grid_id)\n",
    "        location = grid_metadata['location'][j]\n",
    "        tz = grid_metadata['tz'][j]\n",
    "        datetime = pd.to_datetime(\n",
    "            train_labels['datetime'][j],\n",
    "            format=\"%Y%m%dT%H:%M:%S\",\n",
    "            utc=True\n",
    "        )\n",
    "        polygon = grid_metadata['wkt'][j]\n",
    "\n",
    "        if location == 'Delhi':\n",
    "            satellite_metadata_cut = satellite_metadata[satellite_metadata['location'] == 'dl'].copy()\n",
    "        elif location == 'Los Angeles (SoCAB)':\n",
    "            satellite_metadata_cut = satellite_metadata[satellite_metadata['location'] == 'la'].copy()\n",
    "        elif location == 'Taipei':\n",
    "            satellite_metadata_cut = satellite_metadata[satellite_metadata['location'] == 'tpe'].copy()\n",
    "\n",
    "\n",
    "        valid_datetime = [None]*len(satellite_metadata_cut)\n",
    "        satellite_metadata_cut.reset_index(drop=True, inplace=True) # ensure indexes pair with number of rows\n",
    "    #     count = 0\n",
    "        for index, row in satellite_metadata_cut.iterrows():\n",
    "\n",
    "            datetime1 = pd.to_datetime((row['time_start']), format=\"%Y%m%dT%H:%M:%S\", \n",
    "                                    utc=True) \n",
    "            datetime2 = pd.to_datetime(row['time_end'], format=\"%Y%m%dT%H:%M:%S\", \n",
    "                                                utc=True) \n",
    "\n",
    "\n",
    "            truth1 = (datetime <= datetime2)\n",
    "            truth2 = datetime2 <= datetime + timedelta(hours=24)\n",
    " \n",
    "            valid_datetime[index] = (truth1 & truth2)\n",
    "\n",
    "\n",
    "\n",
    "        satellite_metadata_cut['valid_datetime'] = valid_datetime\n",
    "        satellite_metadata_cut = satellite_metadata_cut[satellite_metadata_cut['valid_datetime'] == True]\n",
    "\n",
    "        raw_hdf_set = list(satellite_metadata_cut['granule_id']) \n",
    "\n",
    "\n",
    "        print(raw_hdf_set, location)\n",
    "        print('next:')\n",
    "\n",
    "        list_of_all_AOD_arrays = []\n",
    "        for hdf_filename in raw_hdf_set:\n",
    "            \n",
    "            if training:\n",
    "                filepath = 'train/' + hdf_filename\n",
    "            else:\n",
    "                filepath = 'test/' + hdf_filename\n",
    "\n",
    "            raw_hdf = SD(filepath)\n",
    "\n",
    "            alignment_dict = create_alignment_dict(raw_hdf)\n",
    "\n",
    "            blue_band_AOD = raw_hdf.select(\"Optical_Depth_047\")\n",
    "            name, num_dim, shape, types, num_attr = blue_band_AOD.info()\n",
    "            calibration_dict = create_calibration_dict(blue_band_AOD)\n",
    "            corrected_AOD = calibrate_data(blue_band_AOD, shape, calibration_dict)\n",
    "\n",
    "            xv, yv = create_meshgrid(alignment_dict, shape)            \n",
    "            sinu_crs = Proj(f\"+proj=sinu +R={alignment_dict['crs_params'][0]} +nadgrids=@null +wktext\").crs\n",
    "            wgs84_crs = CRS.from_epsg(\"4326\")            \n",
    "            lon, lat = transform_arrays(xv, yv, sinu_crs, wgs84_crs)\n",
    "\n",
    "            temp = Make_Submatrix(corrected_AOD, lon, lat, alignment_dict, grid_metadata, grid_id)\n",
    "            if len(temp) == 0:\n",
    "                continue\n",
    "            for AOD_array in temp:\n",
    "                list_of_all_AOD_arrays.append(AOD_array)\n",
    "\n",
    "        if list_of_all_AOD_arrays == 0:\n",
    "            if training:\n",
    "                features.append(np.array((np.nan, np.nan, np.nan, np.nan, np.nan)))\n",
    "            else:\n",
    "                features.append(np.array((0, 0, 0, 0, 0)))\n",
    "        else:\n",
    "            features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n",
    "\n",
    "    print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b192e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "1\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "2\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "3\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "4\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "5\n",
      "['20180201T191000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "6\n",
      "['20180202T195000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "7\n",
      "['20180202T195000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "8\n",
      "['20180202T195000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "9\n",
      "['20180202T195000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "10\n",
      "['20180203T203000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "11\n",
      "['20180203T203000_maiac_la_0.hdf'] Los Angeles (SoCAB)\n",
      "next:\n",
      "done\n",
      "12\n",
      "['20180202T032500_maiac_tpe_0.hdf', '20180202T032500_maiac_tpe_1.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180202T032500_maiac_tpe_0.hdf', '20180202T032500_maiac_tpe_1.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180202T032500_maiac_tpe_0.hdf', '20180202T032500_maiac_tpe_1.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180202T032500_maiac_tpe_0.hdf', '20180202T032500_maiac_tpe_1.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180203T041000_maiac_tpe_0.hdf', '20180203T023000_maiac_tpe_0.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180203T041000_maiac_tpe_0.hdf', '20180203T023000_maiac_tpe_0.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20180204T031500_maiac_tpe_0.hdf', '20180204T031500_maiac_tpe_1.hdf'] Taipei\n",
      "next:\n",
      "done\n",
      "19\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_673/3232141249.py:90: UserWarning: Warning: converting a masked element to nan.\n",
      "  features.append(np.array(collect_features(list_of_all_AOD_arrays, 100)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "20\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "21\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "22\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "23\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "24\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "25\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "26\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "27\n",
      "['20180202T064500_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "28\n",
      "['20180203T055000_maiac_dl_0.hdf'] Delhi\n",
      "next:\n",
      "done\n",
      "29\n",
      "['20180203T055000_maiac_dl_0.hdf'] Delhi\n",
      "next:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(grid_metadata['tz'].keys())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m grid_id_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(grid_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 15\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msatellite_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_id_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(train_labels, satellite_metadata, grid_id_list, training)\u001b[0m\n\u001b[1;32m     75\u001b[0m wgs84_crs \u001b[38;5;241m=\u001b[39m CRS\u001b[38;5;241m.\u001b[39mfrom_epsg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4326\u001b[39m\u001b[38;5;124m\"\u001b[39m)            \n\u001b[1;32m     76\u001b[0m lon, lat \u001b[38;5;241m=\u001b[39m transform_arrays(xv, yv, sinu_crs, wgs84_crs)\n\u001b[0;32m---> 78\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mMake_Submatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected_AOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malignment_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mMake_Submatrix\u001b[0;34m(corrected_AOD, lon, lat, alignment_dict, grid_md, gridID)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corrected_AOD[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lon[i,j] \u001b[38;5;241m>\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     p1 \u001b[38;5;241m=\u001b[39m Point(lon[i,j], lat[i,j]) \n",
      "File \u001b[0;32m~/miniforge3/envs/nasa_conda/lib/python3.10/site-packages/shapely/geometry/base.py:475\u001b[0m, in \u001b[0;36mBaseGeometry.bounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbounds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nasa_conda/lib/python3.10/site-packages/shapely/coords.py:190\u001b[0m, in \u001b[0;36mBoundsOp.__call__\u001b[0;34m(self, this)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgeom_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\u001b[38;5;241m.\u001b[39mbounds\n\u001b[0;32m--> 190\u001b[0m cs \u001b[38;5;241m=\u001b[39m lgeos\u001b[38;5;241m.\u001b[39mGEOSGeom_getCoordSeq(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexterior\u001b[49m\u001b[38;5;241m.\u001b[39m_geom)\n\u001b[1;32m    191\u001b[0m cs_len \u001b[38;5;241m=\u001b[39m c_uint(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    192\u001b[0m lgeos\u001b[38;5;241m.\u001b[39mGEOSCoordSeq_getSize(cs, byref(cs_len))\n",
      "File \u001b[0;32m~/miniforge3/envs/nasa_conda/lib/python3.10/site-packages/shapely/geometry/polygon.py:269\u001b[0m, in \u001b[0;36mPolygon.exterior\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_empty()\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexterior\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_empty:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m LinearRing()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Getting data to train the model\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "features = []\n",
    "train_labels = pd.read_csv(\"train_labels.csv\") # Smallest subset\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\", index_col=0)\n",
    "satellite_metadata = pd.read_csv(\"pm25_satellite_metadata.csv\")\n",
    "satellite_metadata = satellite_metadata[satellite_metadata.granule_id.str.endswith('f')]\n",
    "satellite_metadata = satellite_metadata[satellite_metadata['split'] == 'train'].copy()\n",
    "\n",
    "\n",
    "# print(grid_metadata['tz'].keys())\n",
    "grid_id_list = list(grid_metadata['tz'].keys())\n",
    "features = get_features(train_labels, satellite_metadata, grid_id_list, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "pickle.dump( features, open( \"save1.p\", \"wb\" ) )\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae07e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "features_cut = features.copy()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "labels_array = np.array(train_labels.value)\n",
    "cut_labels_array = list(labels_array[0:227].copy())\n",
    "\n",
    "i = 0\n",
    "while( i < len(features_cut)):\n",
    "    print(features_cut[i][0])\n",
    "    if np.isnan(features_cut[i][0]):\n",
    "        features_cut.pop(i)\n",
    "        cut_labels_array.pop(i)\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "print(len(features_cut))\n",
    "print(len(cut_labels_array))\n",
    "\n",
    "def make_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()\n",
    "model.fit(X, Y, epochs=1000, batch_size=10)\n",
    "\n",
    "# estimator = KerasRegressor(build_fn=make_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "# X = np.array(features_cut)\n",
    "# Y = np.array(cut_labels_array)\n",
    "\n",
    "# kfold = KFold(n_splits=10)\n",
    "# results = cross_val_score(estimator, X, Y, n_jobs=1)\n",
    "# print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "predicted = model.predict(np.array(features_cut))\n",
    "print(mean_squared_error(predicted, Y))\n",
    "r2_score(predicted, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87892672",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( model, open( \"model1.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f021451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing the model, first getting features\n",
    "\n",
    "test_labels = pd.read_csv(\"submission_format.csv\") # Smallest subset\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\", index_col=0)\n",
    "satellite_metadata = pd.read_csv(\"pm25_satellite_metadata.csv\")\n",
    "satellite_metadata = satellite_metadata[satellite_metadata.granule_id.str.endswith('f')]\n",
    "satellite_metadata = satellite_metadata[satellite_metadata['split'] == 'test'].copy()\n",
    "\n",
    "\n",
    "grid_id_list = list(grid_metadata['wkt'].keys())\n",
    "\n",
    "features_test = get_features(test_labels, satellite_metadata, grid_id_list, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e23823",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = SD('test/20170109T193000_maiac_la_0.hdf')\n",
    "hdf47 = hdf.select(0)\n",
    "for i in range(4):\n",
    "    plt.imshow(hdf47.get()[i])\n",
    "    plt.show()\n",
    "print(hdf47.get()[0][1199, 1199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfce111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_conda",
   "language": "python",
   "name": "nasa_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
