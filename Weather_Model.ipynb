{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be09f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygrib\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "import keras.backend as backend\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from dateutil import parser\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhdf.SD import SD, SDC, SDS\n",
    "import pyproj\n",
    "from pyproj import CRS, Proj\n",
    "from typing import Union\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca77e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# column_names = ['datetime', 'latitude', 'humidity', 'temperature', 'wind_u', 'wind_v', 'precip']\n",
    "# df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "weather_dict = {}\n",
    "counter = 0\n",
    "\n",
    "def listdirs(rootdir):\n",
    "    global weather_dict\n",
    "    global counter\n",
    "    for it in os.scandir(rootdir):\n",
    "        if it.is_dir():\n",
    "#             print(it.path)\n",
    "            listdirs(it)\n",
    "        \n",
    "        else:\n",
    "#             print(it, ' being added to dict')\n",
    "#             print(type(it))\n",
    "            grbs = pygrib.open(it.path)\n",
    "            for grb in grbs:\n",
    "                quad = (grb['dataDate'], grb['time'], grb['latitudes'], grb['parameterName'])\n",
    "                weather_dict[quad] = [grb['values']]\n",
    "                \n",
    "rootdir = './Actual Weather Data'\n",
    "listdirs(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ef0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_51768/2031586107.py:3: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  print(int(df2[20180202,0, 25.0]['Temperature']))\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(weather_dict)\n",
    "df2 = df.sort_index()\n",
    "print(int(df2[20180202,0, 25.0]['Temperature']))\n",
    "# print(df2)\n",
    "\n",
    "# weather_dict[(20160202,0, 25.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89bec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_features(labels, weather_data_df, grid_id_list, training):\n",
    "    global features\n",
    "    counter = 0\n",
    "    for i in range(len(labels)):\n",
    "        grid_id = list(labels['grid_id'])[i]\n",
    "        j = grid_id_list.index(grid_id)\n",
    "        location = grid_metadata['location'][j]\n",
    "        tz = grid_metadata['tz'][j]\n",
    "        datetime = list(labels['datetime'])[j]\n",
    "        datetime_object = pd.to_datetime(\n",
    "            datetime,\n",
    "            format=\"%Y%m%dT%H:%M:%S\",\n",
    "            utc=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        datetime = datetime.replace('-', '')[:8]\n",
    "        yesterday = (datetime_object - timedelta(hours=24)).strftime('%Y%m%d')\n",
    "        \n",
    "        hours = [0, 600, 1200, 1800]\n",
    "        parameters = ['Relative humidity', 'Temperature', 'u-component of wind', 'v-component of wind', ]\n",
    "       \n",
    "        \n",
    "        if location == 'Delhi':\n",
    "            lat = 28.5\n",
    "        elif location == 'Los Angeles (SoCAB)':\n",
    "            lat = 34\n",
    "        elif location == 'Taipei':\n",
    "            lat = 25\n",
    "\n",
    "        features_to_add = []\n",
    "        for hour in hours:\n",
    "            temp = []\n",
    "            for param in parameters:\n",
    "                temp.append(int(weather_data_df[(int(yesterday), hour, lat\n",
    "                                            )][param]))\n",
    "            features_to_add.append(np.array(temp))\n",
    "            \n",
    "        for hour in hours:\n",
    "            temp = []\n",
    "            for param in parameters:\n",
    "                temp.append(int(weather_data_df[(int(datetime), hour, lat\n",
    "                                            )][param]))\n",
    "            features_to_add.append(np.array(temp))\n",
    "        \n",
    "        features.append(np.array(features_to_add))\n",
    "        counter+=1\n",
    "        if counter%100==0:\n",
    "            print(counter)\n",
    "#         print(features)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfcdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_b(labels, weather_data_df, grid_id_list):\n",
    "    global features_b\n",
    "    for i in range(len(labels)):\n",
    "        grid_id = list(labels['grid_id'])[i]\n",
    "        j = grid_id_list.index(grid_id)\n",
    "        location = grid_metadata['location'][j]\n",
    "        datetime = list(labels['datetime'])[j]\n",
    "        \n",
    "        if (datetime[4:6]) in ['01','02','03']:\n",
    "            temp1 = [1,0,0,0]\n",
    "        elif datetime[4:6] in ['04','05','06']:\n",
    "            temp1 = [0,1,0,0]\n",
    "        elif datetime[4:6] in ['07', '08', '09']:\n",
    "            temp1 = [0,0,1,0]\n",
    "        else:\n",
    "            temp1 = [0,0,0,1]\n",
    "\n",
    "        if location == 'Los Angeles (SoCAB)':\n",
    "            temp2 = [1,0,0]\n",
    "        elif location == 'Delhi':\n",
    "            temp2 = [0,1,0]\n",
    "        else:\n",
    "            temp2 = [0,0,1]\n",
    "        \n",
    "        to_add = temp1 + temp2\n",
    "\n",
    "        features_b.append(to_add)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_51768/2959419267.py:35: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  temp.append(int(weather_data_df[(int(yesterday), hour, lat\n",
      "/var/folders/s9/bqp3gwh57sv7rgc2_ynsbcdm0000gn/T/ipykernel_51768/2959419267.py:42: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  temp.append(int(weather_data_df[(int(datetime), hour, lat\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "features_b = []\n",
    "\n",
    "train_labels = pd.read_csv(\"train_labels.csv\")\n",
    "train_labels['Date'] = pd.to_datetime(train_labels['datetime'], format='%Y-%m-%d')\n",
    "train_labels['Year'] = train_labels['Date'].dt.year\n",
    "\n",
    "train_actual_labels = train_labels[train_labels['Year'] <= 2019].copy()\n",
    "train_cross_val_labels = train_labels[train_labels['Year'] > 2019].copy()\n",
    "\n",
    "# train_actual_labels = train_labels.head(750)\n",
    "# train_cross_val_labels = train_labels.tail(250)\n",
    "\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\", index_col=0)\n",
    "grid_id_list = list(grid_metadata['wkt'].keys())\n",
    "\n",
    "get_weather_features(train_actual_labels, df2, grid_id_list, True)\n",
    "get_features_b(train_actual_labels, df2, grid_id_list)\n",
    "\n",
    "train_features = features.copy()\n",
    "train_features_b = features_b.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import concatenate\n",
    "\n",
    "inputA = Input(shape=(8,4))\n",
    "inputB = Input(shape=7)\n",
    "\n",
    "x = GRU(128, dropout=.20)(inputA)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(.2)(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "y = Dense(64, activation='relu')(inputB)\n",
    "y = Dropout(.2)(y)\n",
    "y = Dense(10, activation='relu')(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "combined = concatenate([x.output, y.output])\n",
    "\n",
    "z = Dense(64, activation='relu')(combined)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(1, activation='linear')(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot\n",
    "# !conda install graphviz\n",
    "from keras.layers import Dropout, GRU, BatchNormalization\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import visualkeras\n",
    "import pydot\n",
    "\n",
    "labels_array = np.array(train_labels.value)[0:750]\n",
    "\n",
    "# def make_model():\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(13, input_dim=32, kernel_initializer='normal', activation='relu'))\n",
    "# #     model.add(Dropout(.2))\n",
    "#     model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "# #     model.add(Dropout(.2))\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "#     # Compile model\n",
    "#     model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     return model\n",
    "\n",
    "# def make_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(GRU(128, input_shape=(8, 4), dropout=.20))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "#     model.add(Dropout(.2))\n",
    "#     model.add(Dense(1))\n",
    "    \n",
    "#     model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#     return model\n",
    "\n",
    "\n",
    "# model = make_model()\n",
    "# plot_model(model)\n",
    "\n",
    "# estimator = KerasRegressor(build_fn=make_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "X = np.array(train_features)\n",
    "X_b = np.array(train_features_b)\n",
    "Y = np.array(labels_array)\n",
    "\n",
    "print(X.shape,X_b.shape,Y.shape)\n",
    "\n",
    "model.fit(x=[X, X_b], y=Y, epochs=100, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating results\n",
    "\n",
    "\n",
    "train_cross_val_labels = train_labels.tail(250)\n",
    "# features = []\n",
    "features_b = []\n",
    "\n",
    "# get_weather_features(train_cross_val_labels, df2, grid_id_list, True)\n",
    "# validation_features = features.copy()\n",
    "\n",
    "get_features_b(train_cross_val_labels, df2, grid_id_list)\n",
    "validation_features_b = features_b.copy()\n",
    "\n",
    "predictions_2019 = model.predict([np.array(validation_features), np.array(validation_features_b)])\n",
    "\n",
    "print(mean_squared_error(np.array(train_cross_val_labels.value), predictions_2019))\n",
    "print(r2_score(train_cross_val_labels.value, predictions_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da234c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging why the model isn't performing as it should\n",
    "\n",
    "for i in range(len(predictions_2019)):\n",
    "    print(i, predictions_2019[i], list(train_cross_val_labels.value)[i], \n",
    "          grid_metadata['location'][list(train_cross_val_labels['grid_id'])[i]], \n",
    "         list(train_cross_val_labels['datetime'])[i])\n",
    "# print(predictions_2019, train_cross_val_labels.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions\n",
    "features = []\n",
    "\n",
    "train_labels = pd.read_csv(\"submission_format.csv\") # Smallest subset\n",
    "grid_metadata = pd.read_csv(\"grid_metadata.csv\", index_col=0)\n",
    "\n",
    "grid_id_list = list(grid_metadata['wkt'].keys())\n",
    "\n",
    "get_weather_features(train_labels, df2, grid_id_list, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa_conda",
   "language": "python",
   "name": "nasa_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
